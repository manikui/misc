# Stanford CS224N: NLP with Deep Learning

## Lecture 1
- We want to represent the meaning of a word
    - Meaning: the idea that is represented by a word, signifier vs. signified
- Usable meaning in a computer: use WordNet, a thesaurus containing lists of synonym sets and hypernyms ("is a" relationships): ```from ntlk.corpus import wordnet```
    - Problems: great as a resource but missing nuance (proficient as a synonym for good), missing new meanings of words (slang), subjective, requires human labor, can't compute word similarity as it has fixed synonym sets
- Traditional NLP: we regard words as just words, discrete symbols - a localist representation where words are represented as one-hot vectors
    - Language has a lot of vocabulary: huge vector dimension
    - We want to understand relationships in the meaning of words e.g. search for "Seattle motel" should match with "Seattle hotel" $\rightarrow$ traditional NLP would have these two vectors as orthogonal, no natural notion of similarity when you have one-hot vectors
        - Could rely on WordNet's list of synonyms but this could fail due to incompleteness or size
    - Solution: learn to encode similarity in the vectors themselves
- Representing words by their **context**: distributional semantics
    - A word's meaning is given by the words that frequently appear close-by
    - If you can explain what context is correct for a certain word, then you understand the meaning of the word
    - When a word $w$ appears in a text, its context is the set of words that appear nearby within a fixed-size window
    - Use the many contexts of $w$ to build up a representation of $w$
- Word vectors: instead of having spare one-hot vectors, we will build dense vectors for each word, chosen so that it is similar to vectors of words that appear in similar contexts - a distribution representation. It will be of a smaller dimension
- Word2vec algorithm: a framework for leaning word vectors
    - We have a large corpus of text. Every word in a fixed vocabulary is represented by a vector. Go through each position $t$ in the text, which has a center word $c$ and context ("outside") words around it $o$
    - Use the similarity of the word vectors for $c$ and $o$ to calculate $P(o | c)$. Keep adjusting the word vectors to maximize this probability
    - Eventually you'll get a word vectors space with distance as a measure of similarity
    - Likelihood $L(\theta) = \Pi_{t = 1}^T \Pi_{-m \leq j \leq m; j \neq 0} P(w_{t + J} | w_t; \theta)$
    - Objective function $J(\theta) = -\dfrac{1}{T} \sum_{t = 1}^T \sum_{-m \leq j \leq m; j \neq 0} \log P(w_{t+j}|w_t; \theta)$
    - Calculate $P(w_{t+j}|w_t; \theta)$ using two vectors per word: $v_w$ when $w$ is the center word and $u_w$ when $w$ is a context word
    - For a center word $c$ and a context word $o$: $P(o|c) = \dfrac{\exp(u_o^Tv_c)}{\sum_{w \in V} \exp(u_w^Tv_c)}$, an example of the softmax function that maps arbitrary values $x_i$ to a probability distribution $p_i$
    - $u_o^Tv_c$ is a measure of similarity between $o$ and $c$ where larger dot product means more similarity and thus larger probability
    - $\sum_{w \in V} \exp(u_w^Tv_c)$ normalizes over entire vocabulary to give probability distribution
    - Optimize using $\theta = \left [v_{\text{aardvark}}, ..., v_{\text{zebra}}, u_{\text{aardvark}}, ..., y_{\text{zebra}} \right ]$. Thus, $\theta \in \mathbb{R}^{2dV}$ for $d$-dimensional vectors and $V$ words
    - $\dfrac{\partial}{\partial v_c} \log P(o | c) = u_o - \sum_{x = 1}^V P(x | c) u_x = u_o - E[x | c]$

## Lecture 2
- Word vector space also has a notion of vector composition, or analogy: "king" vector - "man" vector + "woman" vector = "queen" vector $\rightarrow$ king:man as is queen:woman; australia:beer as is france:champange
- Word2vec aims to provide a reasonably high probability estimate to all words that occur in the context
- Word2vec maximizes objective function by putting similar words nearby in space
- Need to use stochasitc gradient descent as the objective is a function of all windows in the corpus, making it expensive to compute
    - Take a minibatch of words from the corpus and compute the gradient on those
    - End up with a sparse gradient vector as you are only selecting a few words from the entire corpus at each iteration
        - Solution: either use sparse matrix updates to only update certain rows of full embedding matrices U and V, or keep around a hash for word vectors
- Can also capture co-occurence counts: fill a co-occurence matrix X using a window around each word type (word token - fruits instead of a specific fruit), counting up the number of times each word appears within that window
    - X is symmetric and sparse
    - Can measure similarity directly using the counts
    - Problems: X becomes huge with vocabulary increases, sparsity
    - Solution: low rank approximation using SVD that preserves most of the information
- Evaluating word vectors: intrinsic vs. extrinsic
    - Intrinsic: are you guessing the right part of speech? Are you returning the correct synonyms? Evaluation on a specific/intermediate subtask, fast to compute, helps to understand that system
    - Extrinsic: evaluation on a real task (some application that human beings use like web search or phone dialogue), can take a long time to compute accuracy, unclear if the subsystem is the problem or its interaction or other subsystems
- Intrinsic word vector evaluation
    - Word vector analogies: a:b :: c:?
    - Word vector distances and their correlation with human judgements
    - Evaluation word sense: the different contexts and meanings of a word: cluster word windows around words, retrain with each word assigned to multiple different clusters
        - Different sense of a word reside in a linear superposition (weighted sum) in different word embeddings
        - $v_{\text{pike}} = \alpha_1 v_{\text{pike}, 1} + \alpha_2 v_{\text{pike}, 2}$
        - Where $\alpha_i = \dfrac{f_i}{\sum_{j}f_j}$ for frequency $f$

## Lecture 3
- Softmax Classifier: $P(y|x) = \dfrac{\exp(W_y x)}{\sum_c \exp(W_c x)}$
    - Essentially 2 steps: Take the ith row of $W$ and multiply that row with $x$ for all classes, then apply the softmax function with the normalization $\sum_c \exp(W_c x)$ to get a probability
- For each training example $(x, y)$, our objective is to maximize the probability of the correct class $y$
    - Corresponds to: $\max P(y|x) = \max \log(P(y|x)) = \min - \log(P(y|x))$
- Cross entropy loss: for true probability distribution $p$ and computed probability distribution $q$, $H(p, q) = - \sum_c P(c) \log(q(c))$
    - The true probability distribution for some point takes the form: $p = [0, ..., 0, 1, 0, ..., 0]$. Thus, plugging this into the cross-entropy loss yields a single non-zero term: the negative log probability of the true class
    - $H(p, q) = -\log P(y|x)$
- NLP deep learning:
    - Learn both weights $W$ and word vectors $x$
    - Learn both conventional parameters and word representations
    - The word vectors re-represent one-hot vectors - move them around in an intermediate layer vector space
- The loss function directs what the intermediate hidden variables should be, so as to do a good job at predicting the targest for the next layer, etc.
- Neural networks require non-linear functions like sigmoid as with just linear functions, the networks can't model anything more than a linear transformation
    - With more layers and non-linear functions, neural nets can approximate more complex functions
- Named Entity Recognition (NER): finding and classifying names in text (organizations, places, people)
    - Predict entities by classifying words in context (like Word2vec) and then within those extracted entities, combine them into subsequences
        - Problems: boundaries of entity, hard to know if something is an entity ("Future School"), class of unknown/novel entity
- We want to build classifiers of language that work inside a context window of neighboring words
    - Simple solution: average the word vectors in a window and then classify the average vector
        - Problem: loses position information
    - Another solution: train softmax classifier to classify a center word by taking the concatenation of word vectors surrounding it in a window
        - Treat all windows without a named entity in its center as "corrupt"
        - Want a system that returns a high score if there is a named entity at the center 
        - All the neural net does is return an unnormalized score for all combinations of concatenated word vectors
        - The middle layer in the neural net learns non-linear interactions between the input word vectors
        - Whole neural net: $x$ (input: concatenation of word vectors) $\rightarrow h = f(Wx + b) \rightarrow s = u^Th$
        - Example gradient: $\dfrac{\partial s}{\partial b} = \dfrac{\partial s}{\partial h} \dfrac{\partial h}{\partial z} \dfrac{\partial z}{\partial b}= h^T \text{diag}(f'(z)) I = h^T \circ f'(z)$ 
        - Similarly, $\dfrac{\partial s}{\partial W} = \dfrac{\partial s}{\partial h} \dfrac{\partial h}{\partial z} \dfrac{\partial z}{\partial W}$. The first two terms in the chain rule are the same
        - We consider this repetition of gradients as the local error signal $\delta$, the computations in the neural net that are above where $W$ and $b$ are
        - Just compute error signal once, reuse when computing lower-level partial derivatives
        - $\delta = \dfrac{\partial s}{\partial h} \dfrac{\partial h}{\partial z} = h^T \circ f'(z)$
        - Thus: $\dfrac{\partial s}{\partial W} = \dfrac{\partial s}{\partial h} \dfrac{\partial h}{\partial z} \dfrac{\partial z}{\partial W}= \delta \dfrac{\partial z}{\partial W} = \delta^T x^T$

## Lecture 4
- Derivative of a single weight $W_{ij}$: $W_{ij}$ only contributes to one output $z_i$
- $\dfrac{\partial s}{\partial W_{ij}} = \delta_i x_j$, the error signal from above multiplied with the local gradient signal 
- This gives the gradient for the full $W$ as $\delta^T x^T$
- Backpropagation: taking derivatives and using the chain rule while re-using derivatives computed for higher layers in the computation of derivatives for lower layers
    - Forward propagation: expression evaluation of $Wx + b = z \rightarrow f(z) = h \rightarrow u^Th = s$
    - Backpropagation: pass gradients backwards along edges after forward pass
    - Node receives an upstream gradient from the node after it (in terms of the forward pass) - the error signal from above, and it attempts to pass on the correct downstream gradient to the previous node using the local gradient signal and the error signal from above
    - The local gradient signal is determined by the operation contained within the node, i.e. if the node is $h = f(z)$, the local gradient is $\dfrac{\partial h}{\partial z}$
    - The downstream gradient then becomes the upstream gradient (the error signal from above) multipled by the local gradient: $\dfrac{\partial s}{\partial z} = \dfrac{\partial s}{\partial h} \dfrac{\partial h}{\partial z}$
- Backpropagation with multiple downstream gradients to a node: $z = Wx$ - you'll have multiple local gradients that you multiply with the upstream gradient and send them back to their respective inputs: $\dfrac{\partial s}{\partial W} = \dfrac{\partial s}{\partial z}\dfrac{\partial z}{\partial W}$ and $\dfrac{\partial s}{\partial x} = \dfrac{\partial s}{\partial z}\dfrac{\partial z}{\partial x}$
- Backpropagation with multiple upstream gradients to a node: sum the incoming gradients
- Node intuitions:
    - Summing the upstream gradients distribute them across the downstream gradients/branches
    - Max routes them
    - * switches
- Do not calculate upstream gradients that were already completed, just use the upstream gradients entering the current node
- General algorithm:
    - Forwards prop: visit nodes in topological sort order: compute value of current node given predecessors
    - Backwords prop:
        - Initialize output gradients = 1
        - Visit nodes in reverse order:
            - Compute gradient wrt each node using upstream gradient from successors
- Big O() complexity of fprop and bprop should be the same
- Essential to initialize weights to small random values so as to avoid symmetries that hinder learning/specialization
    - Can also use Xavier initialization: variance of random weight distribution inversely proportional to fan-in (previous layer size) and fan-out (next layer size): $\text{Var}(W_i) = \dfrac{2}{n_{\text{in}} + n_{\text{out}}}$
- Use an adaptive optimizer like Adagrad or RMSprop that scale the learning rate and current iteration's gradient by the accumulated gradient

## Lecture 5
- Phrase structure: sentences are built out of units that successively nest (organizes words into nested constitutents: words -> phrases -> bigger phrases -> sentences)
- Determiners/articles: the, a
- Noun: cat, dog
- Context-free grammar rule: Noun phrase (NP) -> Determiner (Det) to Noun (N) or NP -> Det (Adj) N
- Nested constituents: NP -> Det (Adj) N PP; PP -> Prep NP; reusing NP defined previously in a recursive way, making long sentences
- Instead of having phrasal categories, directly represent sentence structure by saying how words depend on (modify or are arguments of) other words
- "Look in the large crate in the kitchen by the door"
    - "Look" is the root of the sentence
    - "in the large crate" is dependent on "look"
    - "the large" is a modifier of "crate", making it dependent on "crate"
    - "in the kitchen" is a modifer of "crate", making it dependent on "crate"
    - "in the" is dependent on "kitchen"
    - "by the door" is a modifer of "crate" as well
    - Identified how different parts of the sentence depend on each other
- Prepositional phrase attachment can create ambiguities: a key parsing decision is how we attach various constituents
    - "Shuttle veteran and longtime NASA executive Fred Gregory appointed to board": is Fred Gregory both or are there two different people being appointed?
- Dependency syntax postulates that syntactic structure consists of relations between lexical items, normally binary asymmetric relations ("arrows") called dependencies
    - The arrows are commonly typed with the name of grammatic relations (subject prepositional object, apposition, etc.)
    - The entity at the top of the arrow is the head, and it is connected to the dependent (modifier)
    - Usually, dependencies form a tree (connected, acyclic, single-head graph)
- Sources of information for dependency parsing:
    - Bilexical affinities: discussion -> issues
    - Dependency distance: mostly with nearby words
    - Intervening materials: dependencies rarely cross intervening verbs or punctuation
    - Valency of heads: figuring out direction of dependency for a head ("was completed" implies only "was" is dependent on "completed", nothing else)
- Dependency parsing:
    - Each word is going to be dependent on another word or is the root (only one root)
    - Don't want cycles, making dependencies a tree
    - Can dependencies (arrows) cross: projective or non-projective?
        - "Ill give a tak tomorrow on bootstrapping": talk -> give; "on bootstrapping" -> "talk"
        - Constituents are delayed to the end of the sentence
- One method: Transition-based parsing or deterministic dependency parsing
    - Greedy choice of attachments guided by good machine learning classifiers
    - "I ate fish"
        - Start: stack([root]) buffer([I, ate, fish])
        - Shift: stack([root, I]) buffer([ate, fish])
        - Shift as "I" is not the head of the sentence: stack([root, I, ate]) buffer([fish])
        - Left-arc reduction: stack([root, ate]) and dependency: ate -> I
        - Shift: stack([root, ate, fish]) buffer([])
        - Right-arc reduction: stack([root, ate]) and dependency: ate -> fish
        - Right-arc reduction: stack([root]) and dependency: root -> ate
        - Finish when buffer is empty
        - Final dependency: root -> ate; ate -> I; ate -> fish
- Improvement: Nivre's MaltParser:
    - To decide whether to shift, left-arc or right-arc, use a discriminative classifier (softmax) 
    - Provides very fast linear time parsing with great performance
- Distributed representations:
    - We represent each word as a d-dimensional dense vector (word embedding) where similar words are expected to have close vectors
    - Add part-of-speech tages (nouns, verbs) and dependency labels as d-dimensional vectors
    - Plural nouns (NNS) should be close to singular nouns (NN); numerical modifiers (num) should be close to adjective modifiers (amod)

## Lecture 6
- Language modeling is the task of predicting what word comes next
    - Given a sequence of words $x^{(1)}, ..., x^{(t)}$, compute the probability distribution of the next word $x^{(t + 1)}$: $P(x^{(t + 1)} | x^{(1)}, ..., x^{(t)})$ where $x^{(t + 1)}$ can be any word in the vocabulary $V$
    - A type of classification task
    - A system that assigns a probability to a piece of text: $P(x^{(1)}, ..., x^{(T)}) = \Pi_{t = 1}^T P(x^{(t)} | x^{(t - 1)}, ..., x^{(1)})$
    - Example: autocorrect, search auto-completing
- Pre-Deep Learning method of learning a language model: n-gram Language Model
    - An n-gram is a chunk of n-consecutive words: unigrams are single words, bigrams are 2-word phrases
    - Idea: collect statistics about how frequent different n-grams are, and use these to predict the next word
- n-gram language models:
    - Simplifying assumption: $x^{(t + 1)}$ depends only on the preceding $n - 1$ words
    - $P(x^{(t+1)} | x^{(t)}, ..., x^{(1)}) = \dfrac{$P(x^{(t+1)}, x^{(t)}, ..., x^{(t - n + 2)})}{P(x^{(t)}, ..., x^{(t - n + 2)})}$, a ratio of the probabilities of a n-gram and a n-1 gram
    - To get these probabilities, count them in some large corpus of text
    - Sparsity problem: if numerator is 0, if we've never seen an event occur in the training data, then our model assigns 0 probability to that event
        - Fix - smoothing: add small $\delta$ to every word in vocabulary, smooths probability distribution
        - Increasing $n$ makes sparsity problem worse, typically $n$ cannot be bigger than 5
        - Another sign of sparsity: not much granularity in the probability distribution when words have very similar probabilities
    - Context problem: If denominator is 0, probability is undefined as the part we are conditioning on does not exist in our training data
        - Fix - back off: condition on less words and shift to a smaller n-gram model
    - Storage: need to store count for all n-grams observed in the corpus (a larger model as $n$ increases)
- 4-gram language model:
    - "As the proctor started the clock, the students opened their ____"
    - Condition only on "students opened their" to predict the next word, discard the rest
    - $P(w | \text{students opened their}) = \dfrac{\text{count}(\text{students opened their w})}{\text{count}(\text{students opened their})}$
    - $P(\text{books} | \text{students opened their}) = 0.4$, $P(\text{exams} | \text{students opened their}) = 0.1$
    - Was it good to discard the beginning part? It provided important context that would have aided our predictions (proctor) -> throwing away too much context reduces predictive power -> sparsity problem
- Using n-gram language models to generate text
    - Say you start with $N$ words, condition on the last $n - 1$ words to determine the nth word
    - Sample one word from the probability distribution generated by the model
    - Then condition on the last $n - 1$ words to determine the next word, etc.
    - Result: grammatical but incoherent - we need to consider more than 3 words at a time but doing so risks introducing sparsity
- Neural Language Models
    - Fixed-window neural Language model:
        - Discard all words except for what we are conditioning on (the fixed window)
        - Represent words as one-hot vectors
        - Look up the word embeddings
        - Pass word embeddings into a hidden layer + bias with a non-linear activation function
        - Pass into output distribution using softmax
        - This returns a probability distribution of all the possible words, select the word with highest density
        - Improvements over n-grams: no sparsity problem, don't need to store all observerd n-grams just the word vectors
        - Problems: fixed window is probably still too small, enlarging window enlarges weight matrix $W$, window can never be large enough
        - Subtle problem: each word $x^{(i)}$ is multiplied by completely different weights in $W$. There is no symmetry in how the inputs are processed, which is not helpful as common word expressions are not being learned -> we need a neural architecture that can process any input length instead of just using a fixed window
- Recurrent Neural Networks (RNN)
    - Input sequence of any length
    - Sequence of hidden states, one for each input
    - Each hidden state $h^{(t)}$ is computed using the previous hidden state $h^{(t - 1)}$ and the input at that step $x^{(t)}$
    - Think of the hidden state as a single state that is mutating over time (time steps) -> apply the same weight matrix $W$ at every step repeatedly
- RNN Language Model
    - ![](https://i.imgur.com/PoESVbU.png)
    - Advantages: can processs any length of input, computation for step $t$ can use information from many steps back, model size doesn't increase for longer input ($W_h, W_e$ remain the same size), same weights applied on every timestep so there's symmetry in how the inputs are processed
    - Disadvantages: recurrent computation is slow as you cannot do them in sequence, difficult to access information from many steps back 
- Training a RNN-LM
    - Get a big corpus of text
    - Feed into RNN-LM, compute output distribution $\hat y^{(t)}$ for every step t -> predict probability distribution of very word, given words so far
    - Loss function on step $t$ is the cross-entropy between predicted probability distribution $\hat y^{(t)}$ and the true next word $y^{(t)}$ (the one-hot vector of $x^{(t + 1)}$
    - Average cross-entropy loss over every step $t$ in corpus gives overall loss for entire training set
    - ![](https://i.imgur.com/8TQqQVv.png)
    - Computing loss and gradients across entire corpus is too expensive, so consider the sequence of words to just be a sentence or a document
    - Also use SGD on a batch of sentences, compute gradient on that batch, update weights, repeat
- Backpropogation for RNNs
    - What is the derivative of the loss at time $t$ wrt the repeated weight matrix $W_h$?
        - The gradient wrt a repeated weight is the sum of the gradient wrt each time it appears
        - Caulculate the gradient wrt each time it appears from back to front ($i = t, ..., 0$), summing gradients as you go -> backpropagation through time
- Generating text with a RNN-LM:
    - ![](https://i.imgur.com/t4Kvl2k.png)
    - Generate text by repeated sampling, sampled output is next step's input
- Evaluating LM
    - Standard evaluation metric - perplexity: inverse proability of corpus, according to the LM
    - For every word in the corpus, compute the product of the inverse of the probability of the next word appearing
    - Perplexity gets larger and larger as corpus gets larger
    - Perplexity = $\exp(J(\theta))$
    - The lower the perplexity the better - want LM to assign high proability to the corpus
- Why do we care about language modeling?
    - It's a benchmark task that helps us measure our progress on understanding language (predicting next word is a general and difficult problem)
    - Subcomponent of many NLP texts, especially those involving generating text or estimating the probability of text: predictive typing, speech recognition, handwriting recognition, spelling/grammar correction

## Lecture 7
- Vanishing gradient:
    - When you want to compute the gradient of the loss at some time step $t$ wrt a hidden layer at an earlier time step $t - 4$, and the intermediate gradients at $t-1$, $t-2$ and $t-3$ are small, then the gradient signal gets smaller and smaller (it accumulates) as it backpropagates further in time
    - The magnitude of the gradient signal from close-by is larger than that from farther away -> hidden layer weights that are close-by have a larger say in loss calculation than farther away ones
    - Model weights are updated only wrt near effects, and less so based on long-term effects
    - Unable to learn a connection between two words that are placed far away
    - We care about $\dfrac{\partial J}{\partial h}$ because the model weights $W$ are a function of the hidden layers $h$
- Effect of vanishing gradient on RNN-LMs:
    - Gradient can be viewed as a measure of the effect of the past on the future
    - If the gradient becomes vanishingly small over longer distances, then we can't tell whether:
        - There's no dependency between step $t$ and $t + n$ in the data (the provided task), there's no connection to be learnt
        - We have wrong parameters to capture the true dependency between $t$ and $t+n$, there is a connection but we are unable to learn it
    - Example: reference to tickets early in a sentence that is key to predicting the next word far later in the sequence
        - The RNN-LM needs to model the dependency between "tickets" on the 7th step and the target word "tickets" far at the end
        - If the gradient is small, the model can't learn this dependency -> it's unable to predict similar long-distance dependencies at test time
    - Example: "The writer of the books __(is/are)__"
        - Correct answer: is
        - Brings up syntactic vs. sequential recency
        - We care about syntactic recency, not sequential as the target word is singular due to the subject being singular. Sequential recency will focus on the plural "books" and select "are"
        - RNN-LM are good at sequential recency due to vanishing gradients, there are weak signals from earlier words that are more important to syntactic recency
- Exploding gradients:
    - If the gradient becomes too big, then the SGD update step becomes too big, drastically changing the model parameters
    - This can cause bad updates: we take too large a step and reach a bad parameter configuration with large loss
    - Worst case: results in Inf or NaN in the network, forcing you to restart training from an earlier checkpoint
- Solution: gradient clipping
    - If the norm of the gradient is greater than some threshold, scale it down before applying SGD update
    - if $||g|| >$ threshold: $g \leftarrow \dfrac{threshold}{||g||} g$
    - Take a truncated step in the same direction
- Solution to vanishing gradient:
    - Main problem of vanishing gradients: it's too difficult for the RNN to learn to preserve information over many timesteps
    - In a vanilla RNN, the hidden state is constantly being rewritten: $h^{(t)} = \sigma \left ( W_h h^{(t - 1)}) + W_x x^{(t)} + b \right )$
    - Non-linearity makes it hard to preserve information from previous steps
    - Can we create a RNN with separate memory? -> LSTM
- Long Short-Term Memory (LSTM)
    - A type of RNN that aims to solve the vanishing gradients problem
    - Architecture:
        - On step $t$, there is a hideen state $h^{(t)}$ and cell state $c^{(t)}$
            - Both are vectors of length $n$
            - The cell stores long-term information
            - The LSTM can erase, write and read information from the cell
        - Selection of which information is erased/written/read is controlled by three corresponding gates
            - Also vectors of length $n$
            - On each timestep, each element of the gates can be open (1), closed (0), or somewhere in-between
            - If gate is open, information is passed through
            - Gates are dynamic, their value is computed based on the current context
            - Forget gate: controls what is kept vs. forgotten from previous cell state: $f^{(t)} = \sigma \left (W_f h^{(t - 1)} + U_f x^{(t)} + b_f \right) \in [0, 1]$
            - Input gate: controls what parts of the new cell content are written to cell: $i^{(t)} = \sigma \left (W_i h^{(t - 1)} + U_i x^{(t)} + b_i \right) \in [0, 1]$
            - Output gate: controls what parts of cell are output to hidden state: $o^{(t)} = \sigma \left (W_o h^{(t - 1)} + U_o x^{(t)} + b_o \right) \in [0, 1]$
            - New cell content: the new content to be written to the cell: $c^{(t)} = \tanh \left (W_c h^{(t - 1)} + U_c x^{(t)} + b_c \right)$ 
            - Update new cell content using gates: forget some content from last cell state $c^{(t - 1)}$ and write some new cell content: $c^{(t)} = f^{(t)} \circ c^{(t - 1)} + i^{(t)} \circ c^{(t)}$
            - Pass cell content through $\tanh$ to get hidden state: $h^{(t)} = o^{(t)} \circ \tanh \left ( c^{(t)} \right )$
            - Note: $\circ$ is element-wise product
    - If the forget gate is set to remember everything on every timestep, then the info in the cell is preserved indefinitely
    - LSTM doesn't guarantee there is no vanishing/exploding gradient, but it provides an easier way for the model to learn long-distance dependencies
    - A lot of extra information to keep track of, but has done well on speech and handwriting recognition tasks
- Gated Recurrent Units (GRU)
    - Simpler alternative to LSTM but allows for long-term signals to persist
    - Architecture:
        - On each timestep $t$ we have input and a hidden state, no cell state
        - Update gate: controls what parts of the previous hidden state are updated vs. preserved: $u^{(t)} = \sigma \left (W_u h^{(t - 1)} + U_u x^{(t)} + b_u \right) \in [0, 1]$
        - Reset gate: controls what parts of the previous hidden state are used to compute new content: $r^{(t)} = \sigma \left (W_r h^{(t - 1)} + U_r x^{(t)} + b_r \right) \in [0, 1]$
        - New hidden state content at timestep: reset gate selects useful parts of previous hudden state. Uses this and current input to compute new hidden state content $t$: $h^{(t)} = \tanh \left (W_h\left(r^{(t)} \circ h^{(t - 1)} \right )  + U_h x^{(t)} + b_h \right)$
        - New hidden state: update gate simultaneouly controsl what is kept from previous hidden state, and what is updated to new hidden state content: $h^{(t)} = \left ( 1 - u^{(t)} \right ) \circ h^{(t - 1)} + u^{(t)} \circ h^{(t)}$, $u^{(t)}$ sets the balance between preserving things from previous hidden state vs. writing new things
    - GRUs make it easier to retain information long-term, by setting the update gate ~ 0
- LSTM vs. GRU:
    - GRU is quicker to compute and has fewer parameters
    - No conclusive endience that one consistently outperforms the other
    - LSTM is a good default choice, especially if your data has particularly long dependencies, or if you have lots of training data as this will help felsh out the parameters morseo than GRU
- Rule of thumb: start with LSTM, switch to GRU for more efficiency
- Vanishing/exploding gradients are also prominent in feed-forward NNs and CNNs
    - Due to chain rule/choice of nonlinearity, gradient can become vanishingly small as it backpropagates
    - Thus, lower layers are learn very slowly, making the NN hard to train
    - Solution: lots of new deep feedforward/CNN architehcures that add more direct connections, thereby alowing the gradient to flow
- Residual connections (ResNet)
    - Employs skip connections that bypass layers, preserving information by default, feeding the "identity connection" directly into a later layer (kind of like an extra bias term but using input from a previous layer)
    - Skip layer is called an identity connection because it directly preserves information
    - Makes deep networks much easier to train
- Dense connections (DenseNet)
    - Directly connect everything to everything
- Highway connections (HighwayNet)
    - Similar to residual connections, but how much of the identity connections factors into the transformation layer is controlled by a dynamic gate
    - Inspired by LSTMs, but applied to deep feed-forward/CNNs
- Though vanishing/exploding gradients are a general problem, RNNs are particularly unstable due to the repeated multiplication by the same weight matrix
- Bidirectional RNNs
    - ![](https://i.imgur.com/zu02hrX.png)
    - ![](https://i.imgur.com/VxdXgEQ.png)
    - Bidirectional RNNs are only application if you have access to the entire input sequence (you should use them by default)
    - They are not applicable to Language Modeling, because you only have the left context available
- Multi-layer RNNs (Stacked RNNs)
    - RNNs are already deep in one dimension (along time)
    - We can also make them "deep" by applying multiple RNNs at rach timestep, allowing the network to compute more complex representations
    - Lower RNNs should compute lower-level features and the higher RNNs should compute higher-level features
    - ![](https://i.imgur.com/vWhBkwf.png)
- Paractical takeaways
    - LSTMs are powerful but GRUs are faster
    - Clip your gradients
    - Use bidrectionality when possible (full input sequence available)
    - Multi-layer RNNs are powerful, but you might need skip/dense connections if it's deep

## Lecture 8
- Machine Translation (MT): translating a sentence x from one language (the source language) to a sentence y in another language (the target language)
- Early MT: rule-based, using a bilingual dictionary to map Russian words to their English counterparts
- Statistical MT: learn a probabilistic model from data
    - We want to find the best English sentence y, given French sentence x: $\arg \max_y P(y|x)$
    - Use Bayes' rule to break this down into two components to be learn separately: $\arg \max_y P(x|y) P(y)$
        - $P(x|y)$: translation model - models how words and phrases should be translated (fidelity), learnt from parallel data
        - $P(y)$: language model - models how to write good English (fluency), learnt from monolingual data
    - To learn translation model:
        - Need large amount of parallel data like pairs of human-translarted French/English sentences (parallel corpus)
        - Consider $P(x, a|y)$ where $a$ is the alignment, i.e. the word-level correspondence between particular words in the translated sentence pair of French sentence $x$ and English sentence $y$
            - Alignment can be many-to-one, one-to-many (fertile - many children), many-to-many (phrase-level translations)
            - Parallel data allows SMT to learn alignment
    - How to compute $\arg \max_y$:
        - We could enumerate every possible $y$ and calculate the probability -> too expensive
        - Solution - decoding: use a heuristic search algorithm to search for the best translation, discarding hypotheses that are too low-probability
    - Huge research field, but the best systems were extremeley complex with many separately-designed subcomponents that required feature engineerings and extra resources
- Neural Machine Translation (NMT)
    - A way to do Machine Translation with a single neural network
    - Used NN architecture called ssequence-to-sequence (seq2seq), which involves two RNNs (Encorder and Decoder RNN)
    - Encoder RNN provides an encoding of the source sentence
    - Decoder RNN is a LM that generates a target sentence, condition on the passed-in encoding (creates a probability distribution of the next possible words)
    - ![](https://i.imgur.com/s6xIzz6.png)
    - Feed source sentence into Encoder RNN, final encoding provides initial hidden state for Decoder RNN
    - At test time: decoder output is fed into next step's input
    - You can use seq2seq for summarization (long text -> short text), dialogue, parsing, code generation
    - Seq2seq is an example of a conditional LM
        - LM because the decoder is predicting the next word of the target sentence
        - Conditional because its predictions are also conditioned on the source sentence
    - NMT directly calculates $P(y|x)$ that SMT had to split into two parts: $P(y|x) = P(y_1|x)P(y_2|y_1, x)...P(y_T|y_1, ..., y_{T - 1}., x)$
    - Training
        - Feed source sentence into encoder RNN
        - Then feed target sentence into decoder RNN using final hidden state of encoder RNN as initial hidden state of decoder
        - For every step of decoder RNN, produce a probability distribution and compute loss
        - Average all of the losses to get total loss for the target sentence
    - Greedy decoding: take most probable word on each step of decoding RNN
        - Problem: taking the $\arg \max$ of a specific word is not the same as the $\arg m\max$ over the whole sentence - it as no way to undo decisions
        - Solution: beam search decoding
            - On each step of decoder, keep track of the k most probable partial translations (hypotheses) where k is the beam size (typically 5-10)
            - A hypothesis $y_1, ..., y_t$ has a score which is its log probability: $\text{score}(y_1, ..., y_t) = \sum_I{i = 1}^t \log P_{LM}(y_i | y_1, ... y_{i - 1}, x)$
            - Scores are all negative so higher score is better
            - We search for high-scoring hypotheses, keeping track of top $k$ on each step
            - Once you have reached the stopping condition, backtrack from last word selected along all words until you reach the start token, using the following score: $\dfrac{1}{t} \sum_I{i = 1}^t \log P_{LM}(y_i | y_1, ... y_{i - 1}, x)$
            - ![](https://i.imgur.com/56nB3vM.png)
            - Stopping condition: different hypotheses may produce end tokens on different timesteps
            - When a hypothesis produces an end token, that hypothesis is complete. Place it aside and continue exploring other hypotheses 
            - Usually, we continue beam search until we reach timestep $T$ or we have at least $n$ completed hypotheses, both of which are pre-defined cutoffs
            - Not guaranteed to find optimal solution, but is more efficient than exhaustive search
    - Advantages of NMT
        - Better performance, more fluent, better use of context
        - It's a single NN that can be optimized end-to-end, without needing to individuallty optimize subcomponents
        - Same method words for all language pairs, as long as you find a good parallel corpus
    - Disadvantages of NMT
        - NMTs are less interpretable: hard to debug
        - Difficult to control: can't specify rules or guidelines to translation (e.g. always translate tahis word in a specific way)
- Evaluating MTs
    - Bilingual Evaluation Understudy (BLEU): compares the machine-written translation to one or more human-written translations, and computes a similarity score based on:
        - n-gram precision by looking at all of the 1,2,3 and 4-grams that appear in each of the translations, and comparing how many of the n-grams that appeared in the machine translation were also present in the human translation 
        - Additional brevity penalty for system translations that are too short
    - BLEU is useful but imperfect
        - There are many valid ways to translate a sentence
        - A good translation can get a poor BLEU score because it has low n-gram overlap with the human translation
- Continuing MT issues 
    - Out-of-vocabulary words
    - Domain mismatch between training and test data
    - Maintaining context over longer text (articles, books, across paragraphs)
    - Low-resource or small corpus language pairs (sometimes reliant on bible translations)
    - Common sense is still hard
    - NMT picks up biases in training data
- Seq2seq: bottleneck problem
    - The last hideen layer in the encoder RNN needs to capture all of the information about the source sentence, as it is going to be passed into the decoder RNN
    - The last sentence forms an information bottleneck
- Solution to the bottleneck problem: Attention
    - On each step of the decoder, use direct connetion to the encoder to focus on a particular part of the source sentence
    - For each decoder step, creae an attention score by taking the dot product with each step in the source sentence
    - Then, take softmax to turn all of the scores into a probability distribution
    - On that decoder step, the probability mass distribution will determine where the encoder RNN will focus
    - Then, use the attention distribution to take a weighted sum of the encoder hidden states. The attention output contains information from the hidden states that received high attention
    - ![](https://i.imgur.com/ITswLgW.png)
    - Concatenate attention output with decoder hidden state, then compute probability distribution to sample next word in the decoder RNN
    - ![](https://i.imgur.com/x5pPiBK.png)
- Attention significantly improves NMT performance
    - Very useful to allow decoder to focus on certain parts of the source
    - Solves the bottleneck problem
    - Helps with vanishing gradient problem
    - Provides interpretability by inspecting attention distribution to see what the decoder was focusing on
    - Get alignment for free: no need to define the notion of alignment as the network just learns it by itself
- Attention is a general Deep Learning technique
    - More general definition of attention: given a set of vector values and a vector query, attention is a technique to compute a weighted sum of the values, dependent on the query
    - The query is attending to the values
    - In seq2seq, each decoder hidden state (query) attends to all of the encoder hidden states at the same time (values)
    - The weighted sum is a selective summary of the information contained in the values. The probability distribution on the values (attention distribution), allows the query to determine which values to focus on
    - A way to obtain a fixed-size representation of an arbitrary set of representations
    - ![](https://i.imgur.com/cVYbu1g.png)
    - ![](https://i.imgur.com/EXLsYii.png)

## Lecture 9
