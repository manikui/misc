# Stanford CS224N: NLP with Deep Learning

## Lecture 1
- We want to represent the meaning of a word
    - Meaning: the idea that is represented by a word, signifier vs. signified
- Usable meaning in a computer: use WordNet, a thesaurus containing lists of synonym sets and hypernyms ("is a" relationships): ```from ntlk.corpus import wordnet```
    - Problems: great as a resource but missing nuance (proficient as a synonym for good), missing new meanings of words (slang), subjective, requires human labor, can't compute word similarity as it has fixed synonym sets
- Traditional NLP (up to 2012): we regard words as just words, discrete symbols - a localist representation where words are represented as one-hot vectors (one 1, many 0s in the vector where 1 = the target word and dimension = size of vocabulary)
    - Language has a lot of vocabulary: huge vector dimension
    - We want to understand relationships in the meaning of words e.g. search for "Seattle motel" should match with "Seattle hotel" $\rightarrow$ traditional NLP would have these two vectors (one for "Seattle", another for "motel") as orthogonal $\Rightarrow$ no natural notion of similarity when you have one-hot vectors
    - ![](https://i.imgur.com/PNGjxen.png)
        - Could rely on WordNet's list of synonyms but this could fail due to incompleteness or size
    - Solution: learn to encode similarity in the vectors themselves
- Representing words by their **context**: distributional semantics
    - A word's meaning is given by the words that frequently appear close-by
    - If you can explain what context is correct for a certain word, then you understand the meaning of the word
    - When a word $w$ appears in a text, its context is the set of words that appear nearby within a fixed-size window
    - Use the many contexts of $w$ to build up a representation of $w$
    - All could be useful to understand the meaning of "banking": ![](https://i.imgur.com/VFnyQMw.png)
- Word vectors: instead of having sparse one-hot vectors (a localist representation of a word), we will build dense vectors for each word (a distributed representation), chosen so that it is similar to vectors of words that appear in similar contexts - a distribution representation. It will be of a smaller dimension
    - All elements in the vector will be non-zero: ![](https://i.imgur.com/pxQzcAT.png)
    - Can then place words within a vector space wherein distance becomes a measure of similarity
    - Dimension of vector space can be determined by a learning algorithm
- Word2vec algorithm: a framework for leaning word vectors
    - We have a large corpus of text. Every word in a fixed vocabulary is represented by a vector. Go through each position $t$ in the text, which has a center word $c$ and context ("outside") words around it $o$
    - Use the similarity of the word vectors for $c$ and $o$ to calculate $P(o | c)$. Keep adjusting the word vectors to maximize this probability
    - Eventually you'll get a word vectors space of some dimension determined by the algorithm with distance as a measure of similarity
    - ![](https://i.imgur.com/JX5fjzX.png)
    - Iterate through each of the words in the sentence
    - Likelihood $L(\theta) = \Pi_{t = 1}^T \Pi_{-m \leq j \leq m; j \neq 0} P(w_{t + J} | w_t; \theta)$
    - Objective function $J(\theta) = -\dfrac{1}{T} \sum_{t = 1}^T \sum_{-m \leq j \leq m; j \neq 0} \log P(w_{t+j}|w_t; \theta)$
    - Calculate $P(w_{t+j}|w_t; \theta)$ using two vectors per word: $v_w$ when $w$ is the center word and $u_w$ when $w$ is a context word
    - For a center word $c$ and a context word $o$: $P(o|c) = \dfrac{\exp(u_o^Tv_c)}{\sum_{w \in V} \exp(u_w^Tv_c)}$, an example of the softmax function that maps arbitrary values $x_i$ to a probability distribution $p_i$ $\Rightarrow$ picking out the words that are most likely to be similar within a distribution of words in the corpus
    - $u_o^Tv_c$ is a measure of similarity between $o$ and $c$ where larger dot product means more similarity and thus larger probability
    - $\sum_{w \in V} \exp(u_w^Tv_c)$ normalizes over entire vocabulary to give probability distribution
    - Optimize using $\theta = \left [v_{\text{aardvark}}, ..., v_{\text{zebra}}, u_{\text{aardvark}}, ..., y_{\text{zebra}} \right ]$. Thus, $\theta \in \mathbb{R}^{2dV}$, containing 2 $d$-dimensional vectors each for $V$ words, which is passed into our likelihood function
    - Slope in our multi-dimensional word vector space, what direction to move in to improve our model's predictive ability: $\dfrac{\partial}{\partial v_c} \log P(o | c) = u_o - \sum_{x = 1}^V P(x | c) u_x = u_o - E[x | c]$ = Difference between actual context word and expected
    - Use gradient descent to update word vectors from their randomized initial states
    - **End result of Word2vec:** a vector space with distance between word vectors indicating similarity, maximizing objective function places similar words nearby

## Lecture 2
- Word vector space also has a notion of vector composition, or analogy: "king" vector - "man" vector + "woman" vector = "queen" vector $\rightarrow$ king:man as is queen:woman; australia:beer as is france:champange
- Word2vec aims to provide a reasonably high probability estimate to all words that occur in a given context
- Word2vec maximizes objective function by putting similar words nearby in space
- Need to use stochastic gradient descent as the objective is a function of all windows in the corpus, making it expensive to compute
    - Take a minibatch of words from the corpus and compute the gradient on those
    - End up with a sparse gradient vector as you are only selecting a few words from the entire corpus at each iteration
        - Solution: either use sparse matrix updates to only update certain rows of full embedding matrices U and V, or keep around a hash for word vectors
- Can also use word vectors derived from co-occurence counts: fill a co-occurence matrix $X$ using a window around each word type (word token - fruits instead of a specific fruit), counting up the number of times each word appears within that window, can skip over common words like "I, and, or, ..."
    - ![](https://i.imgur.com/ZmEXwya.png)
    - $X$ is symmetric and sparse
    - Can measure similarity directly using the vectors' dot product
    - Problems: $X$ becomes huge with vocabulary increases, mostly sparse
    - Solution: low rank approximation using SVD that preserves most of the information in the original matrix (retain only first k singular vectors where $\hat X \in \mathbb{R}^k$ is the best rank-k approximation of $X \in \mathbb{R}^n$)
    - Hacks to improve performance: log transform counts, weight words closer together more, non-negaive Pearson correlation instead of counts
    - Can end up resulting in useful word vectors that are somewhat similar to what you get from Word2Vec
    - Conventional methods can yield useful vector spaces
    - GloVe: ratios of co-occurence probabilities can encode meaning components
        - ![](https://i.imgur.com/0BVKpSO.png)
- Evaluating word vectors: intrinsic vs. extrinsic
    - Intrinsic: are you guessing the right part of speech? Are you returning the correct synonyms? Evaluation on a specific/intermediate subtask, fast to compute, helps to understand that system
    - Extrinsic: evaluation on a real task (some application that human beings use like web search or phone dialogue), can take a long time to compute accuracy, unclear if the subsystem is the problem or its interaction or other subsystems
- Intrinsic word vector evaluation
    - Word vector analogies: a:b :: c:?
    - Word vector distances and their correlation with human judgements
    - Evaluation word sense: the different contexts and meanings of a word: cluster word windows around words, retrain with each word assigned to multiple different clusters
        - Different sense of a word reside in a linear superposition (weighted sum) in different word embeddings
        - $v_{\text{pike}} = \alpha_1 v_{\text{pike}, 1} + \alpha_2 v_{\text{pike}, 2}$
        - Where $\alpha_i = \dfrac{f_i}{\sum_{j}f_j}$ for frequency $f$

## Lecture 3
- Softmax Classifier: $P(y|x) = \dfrac{\exp(W_y x)}{\sum_c \exp(W_c x)}$
    - Essentially 2 steps: Take the ith row of weight matrix $W$ and multiply that row with $x$ for all classes, then apply the softmax function with the normalization $\sum_c \exp(W_c x)$ to get a probability
- For each training example $(x, y)$, our objective is to maximize the probability of the correct class $y$ or equivalently, minimize the negative log probability of the correct class
    - Corresponds to: $\max P(y|x) = \max \log(P(y|x)) = \min - \log(P(y|x))$
- Cross entropy loss: for true probability distribution $p$ and computed probability distribution $q$: $H(p, q) = - \sum_c P(c) \log(q(c))$
    - The true probability distribution for some point takes the form: $p = [0, ..., 0, 1, 0, ..., 0]$. Thus, plugging this into the cross-entropy loss yields a single non-zero term: the negative log probability of the true class
    - $H(p, q) = -\log P(y|x)$
- When classifying over a whole dataset, take the average cross entropy loss
- Why use Neural networks?
    - Need to have classifiers that create non-linear decision boundaries
    - Softmax classifiers create linear decision boundaries whereas neural nets are a family of classifiers that produce non-linear decision boundaries
- Important distinctions from normal ML:
    - Previous discussion about word vectors revealed that we can decide the real number entries within the vector to capture what we are interested in (meaning, analogy, etc.) $\rightarrow$ move the word vector around the vector space to a place that makes sense ("representation learning")
    - Instead of just having to learn a weight matrix $W$, we also get to adjust the inputs (word vectors) $x$
    - Learn both conventional (model, NN) parameters and word representations
- Neural networks basics:
    - Neuron takes in an n-dimensional input vector $x$, multiplies it by a weight vector associated with that specific neuron $w$, sums the scaled output, then adds a bias term $b$. Last, this weighted sum is passed through a non-linear activation function like sigmoid or ReLu to produce the output signal $a$: $a = \frac{1}{1 + \exp \left(-(w^Tx + b) \right)}$
    - Extending this to $m$ different neurons, we have $m$ output activations: ![](https://i.imgur.com/xu4QbmD.png)
    - To simplify, let $z_i = w^{(i)T}x + b^{(i)}$ for the ith neuron. Thus: ![](https://i.imgur.com/QXdUDcf.png)
    - The vector of output activations $a$ can then be written as: ![](https://i.imgur.com/Jo5pJ5L.png) This will be passed onto the next layer
    - The loss function directs what the intermediate hidden variables should be, so as to do a good job at predicting the targest for the next layer, etc.
    - Neural networks require non-linear activation functions like sigmoid as with just linear functions, the networks can't model anything more than a linear transformation
        - With more layers and non-linear functions, neural nets can approximate more complex functions
        - As you add more layers with their own weights $W_1, W_2, ...$, the network will be able to approximate more complex functions and output increasingly non-linear decision boundaries. However, we can still represent these layers as a linear transformation on $x$: $W_1W_2x = W_x$ 
- Named Entity Recognition (NER): finding and classifying names in text (organizations, places, people)
    - Run a classifer that goes through the words one at a time (each word is a word in a context - like Word2Vec), assigning a type to each of them (organization, person, location, etc.)
    - Then the problem becomes picking out the named entities within each of these subsequences, i.e. some named entities have multiple words
        - Problems: boundaries of entity, hard to know if something is an entity ("Future School"), class of unknown/novel entity, entity class is ambiguous and depends on context (Charles Schwab as a person or company)
- We want to build classifiers of language that work inside a context window of neighboring words
    - Simple solution: average the word vectors in a window and then classify the average vector
        - Problem: loses position information, you don't know which of those word vectors is the one you're meant to be classifying
    - Another solution: train softmax classifier to classify a center word by taking the concatenation of word vectors surrounding it in a window, i.e. with window of 5 words, $x = x_{\text{window}} \in \mathbb{R}^{5d}$ and update the word vectors during gradient descent
    - More complicated solution: binary classification with unnormalized scores 
        - Treat all windows without a named entity in its center as "corrupt"
        - Want a system that returns a high score if there is a named entity at the center 
        - All the neural net does is return an unnormalized score for all combinations of concatenated word vectors
        - The middle layer in the neural net learns non-linear interactions between the input word vectors, i.e. if the first word is like "museum" and the second word is a preposition like "in", then it's a good indication that the center word is a location like "Paris"
        - Whole neural net: $x$ (input: concatenation of word vectors) $\rightarrow a = f(Wx + b) \rightarrow s = U^Ta$, the score of whether the center word is a named entity and $U$ is the weight matrix for that final layer
        - Example gradient: $\dfrac{\partial s}{\partial b} = \dfrac{\partial s}{\partial a} \dfrac{\partial a}{\partial z} \dfrac{\partial z}{\partial b}= U^T \text{diag}(f'(z)) I = U^T \circ f'(z)$ 
        - Similarly, $\dfrac{\partial s}{\partial W} = \dfrac{\partial s}{\partial a} \dfrac{\partial a}{\partial z} \dfrac{\partial z}{\partial W}$. The first two terms in the chain rule are the same
        - We consider this repetition of gradients as the local error signal $\delta$, the computations in the neural net that are above where $W$ and $b$ are
        - Just compute error signal once, reuse when computing lower-level partial derivatives
        - $\delta = \dfrac{\partial s}{\partial a} \dfrac{\partial a}{\partial z} = U^T \circ f'(z)$
        - Thus: $\dfrac{\partial s}{\partial W} = \dfrac{\partial s}{\partial a} \dfrac{\partial a}{\partial z} \dfrac{\partial z}{\partial W}= \delta \dfrac{\partial z}{\partial W} = \delta^T x^T$

## Lecture 4
- Derivative of a single weight $W_{ij}$: $W_{ij}$ only contributes to one output $z_i$
- $\dfrac{\partial s}{\partial W_{ij}} = \delta_i x_j$, the error signal from above ($\delta_i$) multiplied with the local gradient signal 
- This gives the gradient for the full $W$ as $\delta^T x^T$
- Now for updating the word vectors:
    - The gradient that arrives at and updates the word vectors can simply be split up for each word vector
    - $\nabla_x J = W^T\delta = \delta_{x_{\text{window}}} \in \mathbb{R}^{5d}$ where $\delta_{x_{\text{window}}, i}$ = $\nabla_{x_{i}}$ for word $i$ in window, the update to that word vector
    - This should push word vectors around so that they will be more helpful in determining named entities
    - Ex: the model can learn that seeing $x_{\text{in}}$ as the word just before the center is indicative of the center word being a location
    - However, it doesn't always work, specifically when synonyms for the word are split between training/test sets
        - While training the model, it will move around the synonym in the training set but the test set stay where they are, resulting in a divergence of meaning wrt the vector space
        - If we had not updated the word vectors while training, assuming they had come from a word embedding sytem beforehand, then the classifier would do a better job
    - What to do:
        - Always use pre-trained word vectors as they are always trained on a huge amount of data - they are already robust to words in/out of your training data
        - Update ("fine tune") your word vectors during training if you have a large dataset
- Backpropagation: taking derivatives and using the chain rule while re-using derivatives computed for higher layers in the computation of derivatives for lower layers
    - Forward propagation: expression evaluation of $z = Wx + b \rightarrow a = f(z) \rightarrow s = U^Ta$
    - Backpropagation: pass gradients backwards along edges after forward pass (using chain rule)
        - Node receives an upstream gradient from the node after it (in terms of the forward pass)
        - This is the error signal from above, and it attempts to pass on the correct downstream gradient to the previous node using the local gradient signal and the error signal from above
        - The local gradient signal is determined by the operation contained within the node, i.e. if the node is $h = f(z)$, the local gradient is $\dfrac{\partial h}{\partial z}$
        - The downstream gradient then becomes the upstream gradient (the error signal from above) multipled by the local gradient: $\dfrac{\partial s}{\partial z} = \dfrac{\partial s}{\partial h} \dfrac{\partial h}{\partial z}$
    - Backpropagation with multiple downstream gradients to a node: $z = Wx$ - you'll have multiple local gradients that you multiply with the upstream gradient and send them back to their respective inputs: $\dfrac{\partial s}{\partial W} = \dfrac{\partial s}{\partial z}\dfrac{\partial z}{\partial W}$ and $\dfrac{\partial s}{\partial x} = \dfrac{\partial s}{\partial z}\dfrac{\partial z}{\partial x}$
    - Backpropagation with multiple upstream gradients returning to a node: sum the incoming gradients
- Node intuitions:
    - Summing the upstream gradients distribute them across the downstream gradients/branches
    - Max routes upstream gradients to only one of the downstream gradients
    - * switches between downstream gradients
- Do not calculate upstream gradients that were already completed, only use the upstream gradient(s) entering the current node, don't bother with further upstream gradients that you should already have calculated
- General algorithm:
    - Initialize all wieghts to small random values
    - Forwards prop: visit nodes in topological sort order: compute value of current node given predecessors, resulting in a singular scalar output (run through operations from first layer to last, outputting loss at the end)
    - Backwards prop (from loss, compute gradients according to operation within each node, going from last layer to first, passing gradients along as you propagate backwards):
        - Initialize output gradients = 1
        - Visit nodes in reverse order:
            - Compute gradient wrt each node using upstream gradient from successors
- Big O() complexity of fprop and bprop should be the same
- NNs have a huge number of weights $\rightarrow$ regularization over all parameters is super important, especially for deep/powerful nets which tend to overfit
    - Powerful NNs can memorize training data if left to train long enough, leading to massive overfitting
- Different non-linear activation functions
    - ![](https://i.imgur.com/iljPtW4.png)
    - Sigmoid was default but no longer en vogue
    - ![](https://i.imgur.com/rLsIp9T.png)
    - Tanh has been foudn to covnerge faster than sigmoid in practice, outputs range from -1 to 1
    - ![](https://i.imgur.com/KRLrOSO.png)
    - Hard Tanh is computationally cheaper than Tanh but saturates for magnitudes of $z > 1$
    - ![](https://i.imgur.com/apfTYyp.png)
    - ![](https://i.imgur.com/aXuPPlg.png)
    - ReLu does not saturate even for larger values of $z$
    - ![](https://i.imgur.com/rU9z3Tb.png)
    - Leaky/Parametric ReLu: ![](https://i.imgur.com/hy8uZJg.png)
        - Traditional ReLu does not propagate any error for non-positive $z$, but Leaky/Parametric ReLu allows for a small proportion of the error to propagate backwards even whn $z$ is negative
        - ![](https://i.imgur.com/G0K2YiJ.png)
- Essential to initialize weights to small random values so as to avoid symmetries that hinder learning/specialization
    - Can also use Xavier initialization: variance of random weight distribution inversely proportional to fan-in (previous layer size) and fan-out (next layer size): $\text{Var}(W_i) = \dfrac{2}{n_{\text{in}} + n_{\text{out}}}$
        - Seeks to keep gradient within region where activation function will pass on gradient, i.e. not too big or small
- Use an adaptive optimizer like Adagrad, RMSprop, Adam (a good place to start) that scale the learning rate and current iteration's gradient by the accumulated gradient
- Learning rate must be correct to an order of magnitude
    - Better results can be obtained by allowing learning rates to decrease per epoch

## Lecture 5
- One of two views of linguistic structure: phrase structure (independent of context)
    - Sentences are built out of units that successively nest
    - They organize words into nested constitutents: words $\rightarrow$ series of words = phrases $\rightarrow$ series of phrases = one bigger phrase $\rightarrow$ series of bigger phrases = a sentence
    - Determiners/articles: the, a
    - Noun: cat, dog
- Context-free grammar (CFG) rules: 
    - Noun phrase (NP) $\rightarrow$ Determiner (Det) to Noun (N) or NP $\rightarrow$ Det (Adj) N
    - Nested constituents: NP $\rightarrow$ Det (Adj) N PP; PP $\rightarrow$ Prep NP; reusing NP defined previously in a recursive way, making long sentences
- Second view of linguistic structure: dependency structure
    - Instead of having phrasal categories (NP, PP, etc.) that are independent of context, directly represent sentence structure by saying how words depend on (modify or are arguments of) other words
    - "Look in the large crate in the kitchen by the door"
        - "Look" is the root of the sentence
        - "in the large crate" is dependent on "look"
        - "the large" is a modifier of "crate", making it dependent on "crate"
        - "in the kitchen" is a modifer of "crate", making it dependent on "crate"
        - "in the" is dependent on "kitchen"
        - "by the door" is a modifer of "crate" as well
        - Fuguring out what words modify other words in the sentence
        - Identifying how different parts of the sentence depend on each other
- Why we care about sentence strucutre
    - Necessary to interpret language correctly
    - Humans communicate complex ideas by composing words together into bigger units
    - We need to know what words are connected to what 
- Prepositional phrase attachment can create ambiguities: a key parsing decision is how we attach various constituents
    - "Shuttle veteran and longtime NASA executive Fred Gregory appointed to board": is Fred Gregory both or are there two different people being appointed?
- Dependency syntax postulates that syntactic structure consists of relations between lexical items, normally binary asymmetric relations ("arrows") called dependencies
    - The arrows are commonly typed with the name of grammatic relations (subject prepositional object, apposition, etc.)
    - The entity at the top of the arrow is the head, and it is connected to the dependent (modifier)
    - Usually, dependencies form a tree (connected, acyclic, single-head graph)
- Sources of information for dependency parsing:
    - Bilexical affinities: discussion -> issues
    - Dependency distance: mostly with nearby words
    - Intervening materials: dependencies rarely cross intervening verbs or punctuation
    - Valency of heads: figuring out direction of dependency for a head ("was completed" implies only "was" is dependent on "completed", nothing else)
- Dependency parsing:
    - Each word is going to be dependent on another word or is the root (only one root)
    - Don't want cycles, making dependencies a tree
    - Can dependencies (arrows) cross: projective or non-projective?
        - "Ill give a tak tomorrow on bootstrapping": talk -> give; "on bootstrapping" -> "talk"
        - Constituents are delayed to the end of the sentence
- One method: Transition-based parsing or deterministic dependency parsing
    - Greedy choice of attachments guided by good machine learning classifiers
    - "I ate fish"
        - Start: stack([root]) buffer([I, ate, fish])
        - Shift: stack([root, I]) buffer([ate, fish])
        - Shift as "I" is not the head of the sentence: stack([root, I, ate]) buffer([fish])
        - Left-arc reduction: stack([root, ate]) and dependency: ate -> I
        - Shift: stack([root, ate, fish]) buffer([])
        - Right-arc reduction: stack([root, ate]) and dependency: ate -> fish
        - Right-arc reduction: stack([root]) and dependency: root -> ate
        - Finish when buffer is empty
        - Final dependency: root -> ate; ate -> I; ate -> fish
- Improvement: Nivre's MaltParser:
    - To decide whether to shift, left-arc or right-arc, use a discriminative classifier (softmax) 
    - Provides very fast linear time parsing with great performance
- Distributed representations:
    - We represent each word as a d-dimensional dense vector (word embedding) where similar words are expected to have close vectors
    - Add part-of-speech tages (nouns, verbs) and dependency labels as d-dimensional vectors
    - Plural nouns (NNS) should be close to singular nouns (NN); numerical modifiers (num) should be close to adjective modifiers (amod)

## Lecture 6
- Language modeling is the task of predicting what word comes next
    - Given a sequence of words $x^{(1)}, ..., x^{(t)}$, compute the probability distribution of the next word $x^{(t + 1)}$: $P(x^{(t + 1)} | x^{(1)}, ..., x^{(t)})$ where $x^{(t + 1)}$ can be any word in the vocabulary $V$
    - A type of classification task
    - A system that assigns a probability to a piece of text: $P(x^{(1)}, ..., x^{(T)}) = \Pi_{t = 1}^T P(x^{(t)} | x^{(t - 1)}, ..., x^{(1)})$
    - Example: autocorrect, search auto-completing
- Pre-Deep Learning method of learning a language model: n-gram Language Model
    - An n-gram is a chunk of n-consecutive words: unigrams are single words, bigrams are 2-word phrases
    - Idea: collect statistics about how frequent different n-grams are, and use these to predict the next word
- n-gram language models:
    - Simplifying assumption: $x^{(t + 1)}$ depends only on the preceding $n - 1$ words
    - $P(x^{(t+1)} | x^{(t)}, ..., x^{(1)}) = \dfrac{$P(x^{(t+1)}, x^{(t)}, ..., x^{(t - n + 2)})}{P(x^{(t)}, ..., x^{(t - n + 2)})}$, a ratio of the probabilities of a n-gram and a n-1 gram
    - To get these probabilities, count them in some large corpus of text
    - Sparsity problem: if numerator is 0, if we've never seen an event occur in the training data, then our model assigns 0 probability to that event
        - Fix - smoothing: add small $\delta$ to every word in vocabulary, smooths probability distribution
        - Increasing $n$ makes sparsity problem worse, typically $n$ cannot be bigger than 5
        - Another sign of sparsity: not much granularity in the probability distribution when words have very similar probabilities
    - Context problem: If denominator is 0, probability is undefined as the part we are conditioning on does not exist in our training data
        - Fix - back off: condition on less words and shift to a smaller n-gram model
    - Storage: need to store count for all n-grams observed in the corpus (a larger model as $n$ increases)
- 4-gram language model:
    - "As the proctor started the clock, the students opened their ____"
    - Condition only on "students opened their" to predict the next word, discard the rest
    - $P(w | \text{students opened their}) = \dfrac{\text{count}(\text{students opened their w})}{\text{count}(\text{students opened their})}$
    - $P(\text{books} | \text{students opened their}) = 0.4$, $P(\text{exams} | \text{students opened their}) = 0.1$
    - Was it good to discard the beginning part? It provided important context that would have aided our predictions (proctor) -> throwing away too much context reduces predictive power -> sparsity problem
- Using n-gram language models to generate text
    - Say you start with $N$ words, condition on the last $n - 1$ words to determine the nth word
    - Sample one word from the probability distribution generated by the model
    - Then condition on the last $n - 1$ words to determine the next word, etc.
    - Result: grammatical but incoherent - we need to consider more than 3 words at a time but doing so risks introducing sparsity
- Neural Language Models
    - Fixed-window neural Language model:
        - Discard all words except for what we are conditioning on (the fixed window)
        - Represent words as one-hot vectors
        - Look up the word embeddings
        - Pass word embeddings into a hidden layer + bias with a non-linear activation function
        - Pass into output distribution using softmax
        - This returns a probability distribution of all the possible words, select the word with highest density
        - Improvements over n-grams: no sparsity problem, don't need to store all observerd n-grams just the word vectors
        - Problems: fixed window is probably still too small, enlarging window enlarges weight matrix $W$, window can never be large enough
        - Subtle problem: each word $x^{(i)}$ is multiplied by completely different weights in $W$. There is no symmetry in how the inputs are processed, which is not helpful as common word expressions are not being learned -> we need a neural architecture that can process any input length instead of just using a fixed window
- Recurrent Neural Networks (RNN)
    - Input sequence of any length
    - Sequence of hidden states, one for each input
    - Each hidden state $h^{(t)}$ is computed using the previous hidden state $h^{(t - 1)}$ and the input at that step $x^{(t)}$
    - Think of the hidden state as a single state that is mutating over time (time steps) -> apply the same weight matrix $W$ at every step repeatedly
- RNN Language Model
    - ![](https://i.imgur.com/PoESVbU.png)
    - Advantages: can processs any length of input, computation for step $t$ can use information from many steps back, model size doesn't increase for longer input ($W_h, W_e$ remain the same size), same weights applied on every timestep so there's symmetry in how the inputs are processed
    - Disadvantages: recurrent computation is slow as you cannot do them in sequence, difficult to access information from many steps back 
- Training a RNN-LM
    - Get a big corpus of text
    - Feed into RNN-LM, compute output distribution $\hat y^{(t)}$ for every step t -> predict probability distribution of very word, given words so far
    - Loss function on step $t$ is the cross-entropy between predicted probability distribution $\hat y^{(t)}$ and the true next word $y^{(t)}$ (the one-hot vector of $x^{(t + 1)}$
    - Average cross-entropy loss over every step $t$ in corpus gives overall loss for entire training set
    - ![](https://i.imgur.com/8TQqQVv.png)
    - Computing loss and gradients across entire corpus is too expensive, so consider the sequence of words to just be a sentence or a document
    - Also use SGD on a batch of sentences, compute gradient on that batch, update weights, repeat
- Backpropogation for RNNs
    - What is the derivative of the loss at time $t$ wrt the repeated weight matrix $W_h$?
        - The gradient wrt a repeated weight is the sum of the gradient wrt each time it appears
        - Caulculate the gradient wrt each time it appears from back to front ($i = t, ..., 0$), summing gradients as you go -> backpropagation through time
- Generating text with a RNN-LM:
    - ![](https://i.imgur.com/t4Kvl2k.png)
    - Generate text by repeated sampling, sampled output is next step's input
- Evaluating LM
    - Standard evaluation metric - perplexity: inverse proability of corpus, according to the LM
    - For every word in the corpus, compute the product of the inverse of the probability of the next word appearing
    - Perplexity gets larger and larger as corpus gets larger
    - Perplexity = $\exp(J(\theta))$
    - The lower the perplexity the better - want LM to assign high proability to the corpus
- Why do we care about language modeling?
    - It's a benchmark task that helps us measure our progress on understanding language (predicting next word is a general and difficult problem)
    - Subcomponent of many NLP texts, especially those involving generating text or estimating the probability of text: predictive typing, speech recognition, handwriting recognition, spelling/grammar correction

## Lecture 7
- Vanishing gradient:
    - When you want to compute the gradient of the loss at some time step $t$ wrt a hidden layer at an earlier time step $t - 4$, and the intermediate gradients at $t-1$, $t-2$ and $t-3$ are small, then the gradient signal gets smaller and smaller (it accumulates) as it backpropagates further in time
    - The magnitude of the gradient signal from close-by is larger than that from farther away -> hidden layer weights that are close-by have a larger say in loss calculation than farther away ones
    - Model weights are updated only wrt near effects, and less so based on long-term effects
    - Unable to learn a connection between two words that are placed far away
    - We care about $\dfrac{\partial J}{\partial h}$ because the model weights $W$ are a function of the hidden layers $h$
- Effect of vanishing gradient on RNN-LMs:
    - Gradient can be viewed as a measure of the effect of the past on the future
    - If the gradient becomes vanishingly small over longer distances, then we can't tell whether:
        - There's no dependency between step $t$ and $t + n$ in the data (the provided task), there's no connection to be learnt
        - We have wrong parameters to capture the true dependency between $t$ and $t+n$, there is a connection but we are unable to learn it
    - Example: reference to tickets early in a sentence that is key to predicting the next word far later in the sequence
        - The RNN-LM needs to model the dependency between "tickets" on the 7th step and the target word "tickets" far at the end
        - If the gradient is small, the model can't learn this dependency -> it's unable to predict similar long-distance dependencies at test time
    - Example: "The writer of the books __(is/are)__"
        - Correct answer: is
        - Brings up syntactic vs. sequential recency
        - We care about syntactic recency, not sequential as the target word is singular due to the subject being singular. Sequential recency will focus on the plural "books" and select "are"
        - RNN-LM are good at sequential recency due to vanishing gradients, there are weak signals from earlier words that are more important to syntactic recency
- Exploding gradients:
    - If the gradient becomes too big, then the SGD update step becomes too big, drastically changing the model parameters
    - This can cause bad updates: we take too large a step and reach a bad parameter configuration with large loss
    - Worst case: results in Inf or NaN in the network, forcing you to restart training from an earlier checkpoint
- Solution: gradient clipping
    - If the norm of the gradient is greater than some threshold, scale it down before applying SGD update
    - if $||g|| >$ threshold: $g \leftarrow \dfrac{threshold}{||g||} g$
    - Take a truncated step in the same direction
- Solution to vanishing gradient:
    - Main problem of vanishing gradients: it's too difficult for the RNN to learn to preserve information over many timesteps
    - In a vanilla RNN, the hidden state is constantly being rewritten: $h^{(t)} = \sigma \left ( W_h h^{(t - 1)}) + W_x x^{(t)} + b \right )$
    - Non-linearity makes it hard to preserve information from previous steps
    - Can we create a RNN with separate memory? -> LSTM
- Long Short-Term Memory (LSTM)
    - A type of RNN that aims to solve the vanishing gradients problem
    - Architecture:
        - On step $t$, there is a hideen state $h^{(t)}$ and cell state $c^{(t)}$
            - Both are vectors of length $n$
            - The cell stores long-term information
            - The LSTM can erase, write and read information from the cell
        - Selection of which information is erased/written/read is controlled by three corresponding gates
            - Also vectors of length $n$
            - On each timestep, each element of the gates can be open (1), closed (0), or somewhere in-between
            - If gate is open, information is passed through
            - Gates are dynamic, their value is computed based on the current context
            - Forget gate: controls what is kept vs. forgotten from previous cell state: $f^{(t)} = \sigma \left (W_f h^{(t - 1)} + U_f x^{(t)} + b_f \right) \in [0, 1]$
            - Input gate: controls what parts of the new cell content are written to cell: $i^{(t)} = \sigma \left (W_i h^{(t - 1)} + U_i x^{(t)} + b_i \right) \in [0, 1]$
            - Output gate: controls what parts of cell are output to hidden state: $o^{(t)} = \sigma \left (W_o h^{(t - 1)} + U_o x^{(t)} + b_o \right) \in [0, 1]$
            - New cell content: the new content to be written to the cell: $c^{(t)} = \tanh \left (W_c h^{(t - 1)} + U_c x^{(t)} + b_c \right)$ 
            - Update new cell content using gates: forget some content from last cell state $c^{(t - 1)}$ and write some new cell content: $c^{(t)} = f^{(t)} \circ c^{(t - 1)} + i^{(t)} \circ c^{(t)}$
            - Pass cell content through $\tanh$ to get hidden state: $h^{(t)} = o^{(t)} \circ \tanh \left ( c^{(t)} \right )$
            - Note: $\circ$ is element-wise product
    - If the forget gate is set to remember everything on every timestep, then the info in the cell is preserved indefinitely
    - LSTM doesn't guarantee there is no vanishing/exploding gradient, but it provides an easier way for the model to learn long-distance dependencies
    - A lot of extra information to keep track of, but has done well on speech and handwriting recognition tasks
- Gated Recurrent Units (GRU)
    - Simpler alternative to LSTM but allows for long-term signals to persist
    - Architecture:
        - On each timestep $t$ we have input and a hidden state, no cell state
        - Update gate: controls what parts of the previous hidden state are updated vs. preserved: $u^{(t)} = \sigma \left (W_u h^{(t - 1)} + U_u x^{(t)} + b_u \right) \in [0, 1]$
        - Reset gate: controls what parts of the previous hidden state are used to compute new content: $r^{(t)} = \sigma \left (W_r h^{(t - 1)} + U_r x^{(t)} + b_r \right) \in [0, 1]$
        - New hidden state content at timestep: reset gate selects useful parts of previous hudden state. Uses this and current input to compute new hidden state content $t$: $h^{(t)} = \tanh \left (W_h\left(r^{(t)} \circ h^{(t - 1)} \right )  + U_h x^{(t)} + b_h \right)$
        - New hidden state: update gate simultaneouly controsl what is kept from previous hidden state, and what is updated to new hidden state content: $h^{(t)} = \left ( 1 - u^{(t)} \right ) \circ h^{(t - 1)} + u^{(t)} \circ h^{(t)}$, $u^{(t)}$ sets the balance between preserving things from previous hidden state vs. writing new things
    - GRUs make it easier to retain information long-term, by setting the update gate ~ 0
- LSTM vs. GRU:
    - GRU is quicker to compute and has fewer parameters
    - No conclusive endience that one consistently outperforms the other
    - LSTM is a good default choice, especially if your data has particularly long dependencies, or if you have lots of training data as this will help felsh out the parameters morseo than GRU
- Rule of thumb: start with LSTM, switch to GRU for more efficiency
- Vanishing/exploding gradients are also prominent in feed-forward NNs and CNNs
    - Due to chain rule/choice of nonlinearity, gradient can become vanishingly small as it backpropagates
    - Thus, lower layers are learn very slowly, making the NN hard to train
    - Solution: lots of new deep feedforward/CNN architehcures that add more direct connections, thereby alowing the gradient to flow
- Residual connections (ResNet)
    - Employs skip connections that bypass layers, preserving information by default, feeding the "identity connection" directly into a later layer (kind of like an extra bias term but using input from a previous layer)
    - Skip layer is called an identity connection because it directly preserves information
    - Makes deep networks much easier to train
- Dense connections (DenseNet)
    - Directly connect everything to everything
- Highway connections (HighwayNet)
    - Similar to residual connections, but how much of the identity connections factors into the transformation layer is controlled by a dynamic gate
    - Inspired by LSTMs, but applied to deep feed-forward/CNNs
- Though vanishing/exploding gradients are a general problem, RNNs are particularly unstable due to the repeated multiplication by the same weight matrix
- Bidirectional RNNs
    - ![](https://i.imgur.com/zu02hrX.png)
    - ![](https://i.imgur.com/VxdXgEQ.png)
    - Bidirectional RNNs are only application if you have access to the entire input sequence (you should use them by default)
    - They are not applicable to Language Modeling, because you only have the left context available
- Multi-layer RNNs (Stacked RNNs)
    - RNNs are already deep in one dimension (along time)
    - We can also make them "deep" by applying multiple RNNs at rach timestep, allowing the network to compute more complex representations
    - Lower RNNs should compute lower-level features and the higher RNNs should compute higher-level features
    - ![](https://i.imgur.com/vWhBkwf.png)
- Paractical takeaways
    - LSTMs are powerful but GRUs are faster
    - Clip your gradients
    - Use bidrectionality when possible (full input sequence available)
    - Multi-layer RNNs are powerful, but you might need skip/dense connections if it's deep

## Lecture 8
- Machine Translation (MT): translating a sentence x from one language (the source language) to a sentence y in another language (the target language)
- Early MT: rule-based, using a bilingual dictionary to map Russian words to their English counterparts
- Statistical MT: learn a probabilistic model from data
    - We want to find the best English sentence y, given French sentence x: $\arg \max_y P(y|x)$
    - Use Bayes' rule to break this down into two components to be learn separately: $\arg \max_y P(x|y) P(y)$
        - $P(x|y)$: translation model - models how words and phrases should be translated (fidelity), learnt from parallel data
        - $P(y)$: language model - models how to write good English (fluency), learnt from monolingual data
    - To learn translation model:
        - Need large amount of parallel data like pairs of human-translarted French/English sentences (parallel corpus)
        - Consider $P(x, a|y)$ where $a$ is the alignment, i.e. the word-level correspondence between particular words in the translated sentence pair of French sentence $x$ and English sentence $y$
            - Alignment can be many-to-one, one-to-many (fertile - many children), many-to-many (phrase-level translations)
            - Parallel data allows SMT to learn alignment
    - How to compute $\arg \max_y$:
        - We could enumerate every possible $y$ and calculate the probability -> too expensive
        - Solution - decoding: use a heuristic search algorithm to search for the best translation, discarding hypotheses that are too low-probability
    - Huge research field, but the best systems were extremeley complex with many separately-designed subcomponents that required feature engineerings and extra resources
- Neural Machine Translation (NMT)
    - A way to do Machine Translation with a single neural network
    - Used NN architecture called ssequence-to-sequence (seq2seq), which involves two RNNs (Encorder and Decoder RNN)
    - Encoder RNN provides an encoding of the source sentence
    - Decoder RNN is a LM that generates a target sentence, condition on the passed-in encoding (creates a probability distribution of the next possible words)
    - ![](https://i.imgur.com/s6xIzz6.png)
    - Feed source sentence into Encoder RNN, final encoding provides initial hidden state for Decoder RNN
    - At test time: decoder output is fed into next step's input
    - You can use seq2seq for summarization (long text -> short text), dialogue, parsing, code generation
    - Seq2seq is an example of a conditional LM
        - LM because the decoder is predicting the next word of the target sentence
        - Conditional because its predictions are also conditioned on the source sentence
    - NMT directly calculates $P(y|x)$ that SMT had to split into two parts: $P(y|x) = P(y_1|x)P(y_2|y_1, x)...P(y_T|y_1, ..., y_{T - 1}., x)$
    - Training
        - Feed source sentence into encoder RNN
        - Then feed target sentence into decoder RNN using final hidden state of encoder RNN as initial hidden state of decoder
        - For every step of decoder RNN, produce a probability distribution and compute loss
        - Average all of the losses to get total loss for the target sentence
    - Greedy decoding: take most probable word on each step of decoding RNN
        - Problem: taking the $\arg \max$ of a specific word is not the same as the $\arg m\max$ over the whole sentence - it as no way to undo decisions
        - Solution: beam search decoding
            - On each step of decoder, keep track of the k most probable partial translations (hypotheses) where k is the beam size (typically 5-10)
            - A hypothesis $y_1, ..., y_t$ has a score which is its log probability: $\text{score}(y_1, ..., y_t) = \sum_I{i = 1}^t \log P_{LM}(y_i | y_1, ... y_{i - 1}, x)$
            - Scores are all negative so higher score is better
            - We search for high-scoring hypotheses, keeping track of top $k$ on each step
            - Once you have reached the stopping condition, backtrack from last word selected along all words until you reach the start token, using the following score: $\dfrac{1}{t} \sum_I{i = 1}^t \log P_{LM}(y_i | y_1, ... y_{i - 1}, x)$
            - ![](https://i.imgur.com/56nB3vM.png)
            - Stopping condition: different hypotheses may produce end tokens on different timesteps
            - When a hypothesis produces an end token, that hypothesis is complete. Place it aside and continue exploring other hypotheses 
            - Usually, we continue beam search until we reach timestep $T$ or we have at least $n$ completed hypotheses, both of which are pre-defined cutoffs
            - Not guaranteed to find optimal solution, but is more efficient than exhaustive search
    - Advantages of NMT
        - Better performance, more fluent, better use of context
        - It's a single NN that can be optimized end-to-end, without needing to individuallty optimize subcomponents
        - Same method words for all language pairs, as long as you find a good parallel corpus
    - Disadvantages of NMT
        - NMTs are less interpretable: hard to debug
        - Difficult to control: can't specify rules or guidelines to translation (e.g. always translate tahis word in a specific way)
- Evaluating MTs
    - Bilingual Evaluation Understudy (BLEU): compares the machine-written translation to one or more human-written translations, and computes a similarity score based on:
        - n-gram precision by looking at all of the 1,2,3 and 4-grams that appear in each of the translations, and comparing how many of the n-grams that appeared in the machine translation were also present in the human translation 
        - Additional brevity penalty for system translations that are too short
    - BLEU is useful but imperfect
        - There are many valid ways to translate a sentence
        - A good translation can get a poor BLEU score because it has low n-gram overlap with the human translation
- Continuing MT issues 
    - Out-of-vocabulary words
    - Domain mismatch between training and test data
    - Maintaining context over longer text (articles, books, across paragraphs)
    - Low-resource or small corpus language pairs (sometimes reliant on bible translations)
    - Common sense is still hard
    - NMT picks up biases in training data
- Seq2seq: bottleneck problem
    - The last hideen layer in the encoder RNN needs to capture all of the information about the source sentence, as it is going to be passed into the decoder RNN
    - The last sentence forms an information bottleneck
- Solution to the bottleneck problem: Attention
    - On each step of the decoder, use direct connetion to the encoder to focus on a particular part of the source sentence
    - For each decoder step, creae an attention score by taking the dot product with each step in the source sentence
    - Then, take softmax to turn all of the scores into a probability distribution
    - On that decoder step, the probability mass distribution will determine where the encoder RNN will focus
    - Then, use the attention distribution to take a weighted sum of the encoder hidden states. The attention output contains information from the hidden states that received high attention
    - ![](https://i.imgur.com/ITswLgW.png)
    - Concatenate attention output with decoder hidden state, then compute probability distribution to sample next word in the decoder RNN
    - ![](https://i.imgur.com/x5pPiBK.png)
- Attention significantly improves NMT performance
    - Very useful to allow decoder to focus on certain parts of the source
    - Solves the bottleneck problem
    - Helps with vanishing gradient problem
    - Provides interpretability by inspecting attention distribution to see what the decoder was focusing on
    - Get alignment for free: no need to define the notion of alignment as the network just learns it by itself
- Attention is a general Deep Learning technique
    - More general definition of attention: given a set of vector values and a vector query, attention is a technique to compute a weighted sum of the values, dependent on the query
    - The query is attending to the values
    - In seq2seq, each decoder hidden state (query) attends to all of the encoder hidden states at the same time (values)
    - The weighted sum is a selective summary of the information contained in the values. The probability distribution on the values (attention distribution), allows the query to determine which values to focus on
    - A way to obtain a fixed-size representation of an arbitrary set of representations
    - ![](https://i.imgur.com/cVYbu1g.png)
    - ![](https://i.imgur.com/EXLsYii.png)

## Lecture 9

## Lecture 10
- Question answering
    - With massive collections of full-text documents, i.e. the web, simply returning relevant documents is of limited use --> we want answers to our questions
    - Requires two parts:
        - Finding documents that might contain an answer - this can be handled by traditional information retrieval/web search
        - Finding an answer **within** a paragraph or a document - often termed reading comprehension
    - Machine comprehension: if the machine can provide a string which speakers would agree both answers that question and does not contain irrelevant information
    - MCTest Corpus: Passage (P) + Question (Q) --> Answer (A), containing stories from which simple questions can be asked and answered
    - 

